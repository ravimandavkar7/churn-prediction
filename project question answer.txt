Q1. How did you deploy your churn model?

Answer:
I deployed my churn prediction model using Streamlit. I trained the model in a Jupyter notebook, saved the trained model, scaler, and feature names using joblib, and then loaded them in a Streamlit app. The app takes user input, applies the same preprocessing steps, and predicts churn probability in real time.

Q2. Why did you save feature_names.pkl?

Answer:
When using one-hot encoding, the number and order of features changes. I saved the feature names from training so that during prediction I can reindex the input data to match the exact feature structure the model was trained on. This avoids feature mismatch errors and ensures consistent predictions.

Q3. Why are you using pd.get_dummies() again in Streamlit?

Answer:
Because the model was trained on one-hot encoded data, the input in production must go through the same encoding. I used pd.get_dummies() in the app to replicate the same transformation applied during training.

Q4. What problem does this line solve?
input_encoded = input_encoded.reindex(columns=feature_names, fill_value=0)


Answer:
This ensures that the input features always match the training feature set. If a category is missing in the current input, it fills that column with zero, and if the order changes, it reorders correctly. This prevents shape mismatch and incorrect predictions.

Q5. Why did you save and reuse the scaler?

Answer:
Scaling changes the distribution of features. If I fit a new scaler in production, the model would receive differently scaled values and predictions would be unreliable. By saving and reusing the same scaler, I ensure consistent feature scaling between training and deployment.

Q6. What kind of error did you face during deployment and how did you fix it?

Answer:
I faced errors like ‚Äúcould not convert string to float‚Äù and feature mismatch issues. I fixed them by applying the same preprocessing steps in Streamlit as in training ‚Äî encoding categorical variables, aligning feature names, and scaling using the saved scaler.
During deployment, I faced an error saying ‚Äúcould not convert string to float‚Äù. This happened because my model was trained on numeric data after encoding, but in the Streamlit app I was initially passing raw categorical values like Contract type = Month-to-month.
To fix this, I made sure that the same preprocessing steps used in training were also applied in the app. I used one-hot encoding on user input, aligned the input features with the original training feature names, and scaled the data using the same scaler that was fitted during training.

This ensured that the model always receives data in the exact format it was trained on, which removed both conversion errors and feature mismatch issues.

Model expects: numbers

App was sending: text
(like "Month-to-month")

So Python said:

I can‚Äôt convert text into numbers ‚Üí error.

Also:

Training had 50+ encoded columns

App input had only 4‚Äì5 columns
‚Üí Feature mismatch.

üü¢ How you fixed it

You did 3 important things:

Encoded the input again

pd.get_dummies(input_df)


Matched the exact feature structure

reindex(columns=feature_names, fill_value=0)


Used the same scaler

scaler.transform(input_encoded)


Q7. Why did you not use a pipeline?

Answer:
In this project, I chose a modular approach by saving preprocessing artifacts separately ‚Äî model, scaler, and feature names ‚Äî and manually applying them in the app. This gave me clear control over each step and helped me understand deployment issues like feature mismatch and scaling consistency.
In this project, I decided not to use a Scikit-learn pipeline and instead handled preprocessing steps separately. I saved the trained model, the scaler, and the feature names as individual artifacts and applied them step by step in the Streamlit app.

This modular approach helped me clearly understand how each stage of the ML workflow works ‚Äî especially encoding, feature alignment, and scaling. It also made it easier to debug deployment issues like feature mismatch and data format errors.

Once the solution became stable, I realized that this approach works well for learning and debugging, and in future iterations I would convert it into a single pipeline for cleaner production deployment.

What this really means (simple words)

You are saying three smart things:

1Ô∏è‚É£ You had full control

Instead of:

Everything hidden inside a pipeline

You could clearly see:

Where encoding happens

Where scaling happens

Where prediction happens

This helped you learn faster.

2Ô∏è‚É£ You could debug easily

When errors came:

You knew:

Is it encoding problem?

Is it scaling problem?

Is it feature order problem?

With a pipeline, beginners sometimes don‚Äôt understand where the issue is.
Your approach helped you solve real deployment errors.


Q8. How did you make predictions business-friendly?

Answer:
Instead of only showing probabilities, I categorized customers into High, Medium, and Low risk using calibrated thresholds. This helps business teams quickly identify which customers need immediate retention action.

Q9. What is the difference between your dashboard insights and model predictions?

Answer:
The dashboard shows overall trends, like low-tenure customers having higher churn. The model predicts churn probability for individual customers by combining all features together. Both are complementary ‚Äî dashboards explain patterns, models support decisions at the customer level.

Q10. What did you learn from deploying this project?

Answer:
I learned the importance of consistent preprocessing between training and production. Even a good model can fail if encoding, scaling, or feature order is inconsistent. Deployment taught me how to build reliable end-to-end ML solutions.